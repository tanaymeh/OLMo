import lance

import numpy as np

import torch
from torch.utils.data import Dataset

from typing import Dict, List, Tuple, Optional, Union, Any

from ..aliases import PathOrStr

__all__ = ["LanceDataset"]
ListOrStr = Union[str, List]


class LanceDataset(Dataset[Dict[str, Any]]):
    """
    A PyTorch :class: `~torch.utils.data.Dataset` backed by a lance dataset
    of token IDs. Token IDs are chunked together into contiguous blocks of ``chunk_size``
    to create instances.

    :param dataset: String path to the lance dataset
    :param chunk_size: The number of tokens to chunk together into a single instance.
        Generally this should correspond to your model's maximum input length.
    :param generate_attention_mask: If ``True``, each instance returned from ``__getitem__`` will include an
        attention mask generated by masking each padding token.
    :param pad_token_id: The ID of the padding token. Required if ``generate_attention_mask`` is ``True``.
    :param label_mask_dataset: Optional string path to boolean lance dataset of label masks.
    """
    def __init__(
        self,
        dataset,
        chunk_size: int = 1024,
        generate_attention_mask: bool = True,
        pad_token_id: Optional[int] = None,
        label_mask_dataset = None
    ):
        if not dataset:
            raise ValueError("At least one lance dataset is required")
        
        if generate_attention_mask and not pad_token_id:
            raise ValueError("'pad_token_id' is required for 'generate_attention_mask'")
        
        self.dataset = lance.dataset(dataset)
        self.label_mask_dataset = lance.dataset(label_mask_dataset) if label_mask_dataset else None
        self._chunk_size = chunk_size
        self._generate_attention_mask = generate_attention_mask
        self._pad_token_id = pad_token_id

        if label_mask_dataset:
            if not self.dataset.count_rows() == self.label_mask_dataset.count_rows():
                raise ValueError("Token Dataset and Label mask dataset must have the same number of tokens")
            
    @property
    def chunk_size(self) -> int:
        return self._chunk_size
    
    @property
    def max_seq_len(self) -> int:
        # For compatibility with composer's SpeedMonitor callback
        return self._chunk_size
    
    def __len__(self) -> int:
        return self.dataset.count_rows() - self.chunk_size
    
    def _from_idxs(self, idxs):
        """Function to retrieve tokens at specific indices from the Lance dataset"""
        data = self.dataset.take(idxs).to_pylist()
        data = torch.tensor(list(map(lambda x: x['value'], data)))
        return data

    