import lance

import numpy as np

import torch
from torch.utils.data import Dataset

from typing import Dict, List, Tuple, Optional, Union, Any

from ..aliases import PathOrStr

__all__ = ["LanceDataset"]
ListOrStr = Union[str, List]


class LanceDataset(Dataset[Dict[str, Any]]):
    """
    A PyTorch :class: `~torch.utils.data.Dataset` backed by one or more lance datasets
    of token IDs. Token IDs are chunked together into contiguous blocks of ``chunk_size``
    to create instances.

    No special tokens are added to the input IDs so it's assumed that if you want
    EOS tokens between documents, for example, those will already be in the lance dataset.

    :param paths: Paths to lance datasets.
    :param chunk_size: The number of tokens to chunk together into a single instance.
        Generally this should correspond to your model's maximum input length.
    :param lance_dtype: The numpy datatype of the lance dataset
    :param metadata: Metadata to add to each item. This should be a dictionary or a list of dictionaries
        with the same number of items as there are paths.
    :param include_instance_metadata: If ``True`` (the default), each instance returned from `__getitem__` will
        include the metadata from its source.
    :param generate_attention_mask: If ``True``, each instance returned from ``__getitem__`` will include an
        attention mask generated by masking each padding token.
    :param pad_token_id: The ID of the padding token. Required if ``generate_attention_mask`` is ``True``.
    :param label_mask_paths: Optional paths to ``np.bool_`` lance dataset of label masks.
    """

    def __init__(
        self,
        *paths: ListOrStr,
        chunk_size: int = 1024,
        lance_dtype=np.uint16,
        metadata: Optional[Union[List[Dict[str, Any]], Dict[str, Any]]] = None,
        include_instance_metadata: bool = True,
        generate_attention_mask: bool = False,
        pad_token_id: Optional[int] = None,
        label_mask_paths: Optional[List[PathOrStr]] = None,
    ):
        if not paths:
            raise ValueError("At least one path is required")

        if generate_attention_mask and not pad_token_id:
            raise ValueError("'pad_token_id' is required for 'generate_attention_mask'")

        if label_mask_paths and len(label_mask_paths) != len(paths):
            raise ValueError("There must be the same number of 'label_mask_paths' as there are 'paths'")

        if isinstance(metadata, list):
            if len(metadata) != len(paths):
                raise ValueError("'metadata' should have the same length as the number of file paths")
            else:
                metadata = [metadata or {}] * len(paths)

        self._lance_paths = paths
        self._metadata = metadata
        self._label_mask_paths = label_mask_paths
        self._chunk_size = chunk_size
        self._mmap_offsets: Optional[List[Tuple[int, int]]] = None
        self._num_instances: Optional[int] = None
        self.dtype = lance_dtype
        self._include_instance_metadata = include_instance_metadata
        self._generate_attention_mask = generate_attention_mask
        self._pad_token_id = pad_token_id

        @property
        def chunk_size(self) -> int:
            return self._chunk_size

        @property
        def max_seq_len(self) -> int:
            # For compatibility with composer's SpeedMonitor callback.
            return self.chunk_size

        def datasets(self) -> List:
            if isinstance(self._lance_paths, str):
                return [lance.dataset(self._lance_paths)]
            else:
                return [lance.dataset(path) for path in self._lance_paths]

        def from_indices(self, dataset: lance.dataset, idxs: List, col_name: str = "value") -> List:
            data = dataset.take(idxs).to_pylist()
            data = list(map(lambda x: x[col_name], data))
            return data

        def _read_chunk_from_lance(self, dataset: lance.dataset, index: int, dtype=None) -> torch.Tensor:
            # Define the indices in the chunk we wish to load and load it
            chunk_indices = np.arange(index, index + self._chunk_size)
            chunk = self.from_indices(dataset, chunk_indices, col_name="value")
            if dtype == np.bool_:
                return torch.tensor(chunk)
            else:
                return torch.tensor(chunk.astype(np.int_), dtype=torch.long)

        def __len__(self):
            # So we don't run into Index OOB errors
            return self.datasets[0].count_rows() - self._chunk_size
        
        def 


class GPTDataset(Dataset):
    def __init__(self, dataset_path, context_len):
        # Load the lance dataset from the saved path
        self.ds = lance.dataset(dataset_path)
        self.context_len = context_len
        # Doing this so the sampler never asks for an index at the end of text
        self.length = self.ds.count_rows() - context_len

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        """
        Generate a list of indices starting from the current idx to idx+context_len+1
        Use the from_idxs function to get data in said indexes and then divide it into features (x) and target (y)
        """
        current_window_idxs = np.arange(idx, idx + self.context_len + 1)
        data = self.from_idxs(current_window_idxs)
        x = data[0 : self.context_len]
        y = data[1 : self.context_len + 1]  # +1 because our target is the sentence is 1 step ahead of input text
        return x, y
